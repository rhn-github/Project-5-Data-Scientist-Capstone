{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Loading data, preliminary study of content](#load)\n",
    "* [Summary of Data Content](#summary1)\n",
    "* [Cleaning Data](#clean)\n",
    "* [Exploratory Data Analysis](#explore1)\n",
    "* [Aggregating the Data](#aggregate)\n",
    "* [Exploratoring Aggregated Data](#explore2)\n",
    "* [Feature Engineering](#feature)\n",
    "* [Modeling](#model)\n",
    "    - [1. Copy dataframe; Apply Train Test Split](#copy)\n",
    "    - [2. Build Pipelines](#pipes)\n",
    "    - [3. Train Base Models](#train)\n",
    "    - [4. Evaluate First Predictions](#evaluate)\n",
    "    - [5. Tune Models and Re-Compute Accuracy](#tune)    \n",
    "* [Recommendation](#recomend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. \n",
    "The dataset was loaded and cleaned, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, preliminary study of content<a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path = \"mini_sparkify_event_data.json\"\n",
    "event_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study schema\n",
    "event_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample check columns 1 thru 10 for content\n",
    "event_log.select('artist','auth','firstName','gender','itemInSession','lastName','length','level','location','method').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample check columns 11 thru 18 for content\n",
    "event_log.select('page','registration','sessionId','song','status','ts','userAgent','userId').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "event_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe columns 1 thru 6 for stats\n",
    "event_log.describe('artist','auth','firstName','gender','itemInSession','lastName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe columns 7 thru 12 for stats\n",
    "event_log.describe('length','level','location','method','page','registration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe columns 13 thru 18 for stats\n",
    "event_log.describe('sessionId','song','status','ts','userAgent','userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish unique values in page column\n",
    "event_log.select(\"page\").dropDuplicates().sort(\"page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish unique values in level column\n",
    "event_log.select(\"level\").dropDuplicates().sort(\"level\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish unique values in auth column\n",
    "event_log.select(\"auth\").dropDuplicates().sort(\"auth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relating method to activity\n",
    "event_log.select(\"method\",\"page\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify relationship between page, registration, sessionId, itemInSession, ts and userId\n",
    "event_log \\\n",
    "    .filter(f.col('userId')==30)\\\n",
    "    .select('page','registration','sessionId','itemInSession','ts','userId') \\\n",
    "    .sort(f.asc('sessionId'),f.asc('ts')) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify relationship between registration, ts and userId\n",
    "event_log \\\n",
    "    .groupBy('registration','userId').agg(f.min('ts').alias('ts')) \\\n",
    "    .sort(f.asc('userId')) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate unixtimestamps ts and and registration into a date format\n",
    "event_log = event_log \\\n",
    "    .withColumn('ts_date', f.to_utc_timestamp(f.from_unixtime(f.col('ts')/1000,'yyyy-MM-dd HH:mm:ss'),'EST')) \\\n",
    "    .withColumn('reg_date', f.to_utc_timestamp(f.from_unixtime(f.col('registration')/1000,'yyyy-MM-dd HH:mm:ss'),'EST'))\n",
    "\n",
    "event_log.select('ts_date','reg_date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first and last ts_dates, first Registration date\n",
    "event_log.agg(\n",
    "    f.min('ts_date').alias('start'),\n",
    "    f.max('ts_date').alias('end'),\n",
    "    f.min('reg_date').alias('first_reg')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Content:<a name=\"summary1\"></a>\n",
    "#### Scope:\n",
    "* Checking the eraliest and latest timespamps shoes that data was being collected between 01/10/2018 and 03/12/2018, just over 2 months.\n",
    "* The first registration was showing as 18/03/2018.\n",
    "\n",
    "#### Dataframe contains the following columns:\n",
    "**1. Data relating to users**:\n",
    "* `userId` - a unique numeric identifier assigned to each user who is registered with the platform.\n",
    "* `firstName` - users first name.\n",
    "* `lastName` - users surname.\n",
    "* `gender` - their gender.\n",
    "* `location` - the location of their registered address.\n",
    "* `level` - the tier *paid* or *free* to which they are subscribed.\n",
    "\n",
    "**2. Data relating to sessions (periods of time for which a user is logged in to the platform)**:\n",
    "* `sessionId` - the id number for the session. Each session for a particlular user has its own id. THe same sessionId may appear in another users account. \n",
    "* `auth` - an identifier for the authorisation level for the session, one of four possibilities:- *Cancelled, Guest, Logged In, Logged Out.* No userIds are associated with *Logged Out* or *Guest*.\n",
    "* `userAgent` - The browser / operating system used to access the platform.\n",
    "* `registration` - the timestamp associated with the time when the user registered. Events triggered by individuals who are not logged in return no registration timestamp.\n",
    "\n",
    "**3. Event logs** are recorded for individual interactions between a user and the platform:\n",
    "* `page` - the type of event. There are 22 different page types such as *NextSong* for playing a song, *Add to Playlist* for adding a song to a playlist, *Home* for visiting the platform's home-page etc.\n",
    "* `method` - User interaction with platform: *Put* for input comming from user, *Get* for output recieved by user.\n",
    "* `itemInSession` - sequential number with in a session for an event that was logged.\n",
    "* `ts` - an integer timestamp for recording the time of an event in milliseconds where ts=0 is defined 01.01.1970 00:00:00.000).\n",
    "\n",
    "**4. Data relating to songs played**, the event type (page) NextSong records that the user is playing a song. For this event song details are recorded:\n",
    "* `song` - the song's title.\n",
    "* `length` - the length of time for which the song was listened to.\n",
    "* `artist` - the song's performer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data<a name=\"clean\"></a>\n",
    "\n",
    "From the preliminary study of the data and its statistics, the following data quality issues were identified:\n",
    "\n",
    "#### Issues:\n",
    "* Min Value for `userId` is empty.<br>\n",
    "* Missing values in `artist`, `firstName`, `gender`, `lastName`, `length`, `location`, `registration`, `song` & `userAgent`\n",
    "\n",
    "#### Fixes:\n",
    "* Create new dataframe `event_log_valid`<br>\n",
    "* Drop Nulls from `userId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null as \"userId\"\n",
    "event_log_valid = event_log.dropna(how = \"any\", subset = [\"userId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recount rows\n",
    "event_log_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It would appear that dropping null userId had no effect; so content of \"userId\" was rechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log_valid.select(\"userId\").dropDuplicates().sort(\"userId\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine Issue\n",
    "* No nulls but an empty String<br>\n",
    "#### Redefine Fix\n",
    "* Drop empty string<br>\n",
    "* Revalidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with empty string as \"userId\"\n",
    "event_log_valid = event_log_valid.filter(event_log_valid[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recount rows\n",
    "event_log_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe columns 1 thru 6 for stats\n",
    "event_log_valid.describe('artist','auth','firstName','gender','itemInSession','lastName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe columns 7 thru 12 for stats\n",
    "event_log_valid.describe('length','level','location','method','page','registration','sessionId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe columns 13 thru 12 for stats\n",
    "event_log_valid.describe('song','status','ts','userAgent','userId').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State of Data after Cleaning\n",
    "* all columns refering to song information, `song`, `artist`, `length` contain 228108 records.\n",
    "* all other columns contain 278154 records.\n",
    "* this is plausible, because songs are only played when the event (page) `NextSong` is logged. For all other log events (pages) there would be no song infomation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis<a name=\"explore1\"></a>\n",
    "Performing EDA with the cleaned subset of the data and doing basic manipulations within Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pareto analysis of `Page` types\n",
    "* the column `page` is one of the most useful for our prediction in that it contains information about the type of log event that occurred. This information can be transformed to provide data about the users activity, which in turn can be used to predict the if a user is going to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of unique page values to control output of .show()\n",
    "pages = event_log_valid.select(\"page\").dropDuplicates().count()\n",
    "# obtain an aggregated count of each page type, in descending order\n",
    "pages_agg = event_log_valid \\\n",
    "    .groupby('page') \\\n",
    "    .agg(f.count('page')).withColumnRenamed('count(page)','count') \\\n",
    "    .sort(f.desc('count'))\n",
    "# send to pandas for visualisation\n",
    "pages_agg_pd = pages_agg.toPandas()\n",
    "# create, show and save as *png pareto diagram for count of page events\n",
    "pages_agg_pd = pages_agg_pd.sort_values('count',ascending=False).reset_index()\n",
    "plt.figure(figsize = [10, 5])\n",
    "sb.barplot(y='page', x='count', data=pages_agg_pd, order=pages_agg_pd['page'], color = sb.color_palette()[0])\\\n",
    ".set_title('Pareto of Page Types', fontsize=12)\n",
    "plt.ylabel('')   \n",
    "plt.savefig('pages_pareto.png')\n",
    "plt.show();     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* it can be seen that the page with by far the most event logs is `NextSong`, which is to be expected, after all the purpose of Sparkify is to play songs.\n",
    "* however `NextSong` is so dominant, it is hard to see the results at the tail end of the Pareto, so it will be replotted without NextSong to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redraw, show and save as *.png Pareto without NextSong to better visuzalise tail of pareto\n",
    "pages_agg_pd = pages_agg_pd.loc[pages_agg_pd.page != 'NextSong'].sort_values('count',ascending=False).reset_index()\n",
    "plt.figure(figsize = [10, 5])\n",
    "sb.barplot(y='page', x='count', data=pages_agg_pd, order=pages_agg_pd['page'], color = sb.color_palette()[0])\\\n",
    ".set_title('Pareto of Page Types without NextSong', fontsize=12)\n",
    "plt.ylabel('')   \n",
    "plt.savefig('pages_pareto_2.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* it is interesting to see that `Cancel` and `Cancellation Confirmation`, the two events that we wish to predict are virtually negligible compared to the others, and that with this sample data set the number of `downgrade` and `Submit Downgrade` events do not match, like as much the numbers of `upgrade` and `Submit Upgrade` events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Gender difference for Number of Users and Webpage Usage\n",
    "* another insight that was studied in the exploratory analysis is the difference between genders, in terms of number of users and amount of usage per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of number of users by gender and rate of visits by gender\n",
    "# obtain aggregated count of number of users, number of sessions by gender \n",
    "# to give number of sessions per user by gender\n",
    "gender_agg = event_log_valid \\\n",
    "    .groupby('gender') \\\n",
    "    .agg(\n",
    "        f.count('sessionId').alias('visits'),\n",
    "        f.countDistinct('userId').alias('users')\n",
    "    ) \\\n",
    "    .withColumn('visits_per', f.round(f.col('visits')/f.col('users'))) \n",
    "# send to pandas for visualisation\n",
    "gender_agg_pd = gender_agg.toPandas()\n",
    "# create, show and save as *png barcharts comparing users by gender and usage by gender\n",
    "plt.figure(figsize = [12,4])\n",
    "# User Count\n",
    "plt.subplot(1,2,1)\n",
    "sb.barplot(x=\"gender\", y=\"users\", data=gender_agg_pd).set_title('Number of Users by Gender', fontsize=12)\n",
    "tick_names = ['Female','Male']\n",
    "plt.xticks([0,1],tick_names)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('count')\n",
    "# Usage\n",
    "plt.subplot(1,2,2)\n",
    "sb.barplot(x=\"gender\", y=\"visits_per\", data=gender_agg_pd).set_title('Rate of Usage by Gender', fontsize=12)\n",
    "tick_names = ['Female','Male']\n",
    "plt.xticks([0,1],tick_names)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Sessions / User')\n",
    "plt.savefig('user_gender.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* whilst there are clearly more male users than female, it is interesting to see that female users spend more time using Sparkify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Gender difference in terms of Paid Subscription Level\n",
    "* finally the difference between the genders regarding the proportion of users who at somepoint had Paid Subscription was measured (including even those who subsequently downgraded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of % of users who have had or who have Paid level\n",
    "# obtain aggregated count of number of users who have logs at Paid level,\n",
    "# join in the total number users from gender_agg to obtain % users who have logs at paid level\n",
    "gender_level_agg = event_log_valid \\\n",
    "    .filter(f.col('level')=='paid') \\\n",
    "    .groupby('gender','level','userId') \\\n",
    "    .agg(\n",
    "        f.countDistinct('userId').alias('count')\n",
    "    ) \\\n",
    "    .groupby('gender','level') \\\n",
    "    .agg(\n",
    "        f.sum('count').alias('count')\n",
    "    ) \\\n",
    "    .join(gender_agg.select('gender','users'), on = 'gender', how = 'left') \\\n",
    "    .withColumn('paid_per',100*f.col('count')/f.col('users')) \\\n",
    "# send to pandas for visualisation\n",
    "gender_level_agg_pd = gender_level_agg.toPandas()\n",
    "# create, show and save as *png bar chart for % users who have logs at paid level\n",
    "plt.figure(figsize = [6,4])\n",
    "sb.barplot(x=\"gender\", y=\"paid_per\", data=gender_level_agg_pd)\\\n",
    ".set_title('Users who use or have used the Paid Level', fontsize=12)\n",
    "tick_names = ['Female','Male']\n",
    "plt.xticks([0,1],tick_names)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('% Users')\n",
    "plt.savefig('level_gender.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a marginally higher proportion of female users have at some point had a paid subscription, compared to male users.\n",
    "* we can conclude from this that gender is an important factor to consider when looking to predict user behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating the data to create a new dataframe containing one row per UserId<a name=\"aggregate\"></a>\n",
    "* the data as supplied is essentially a table of event logswhere each row represents a single user event.\n",
    "* the purpose of this exercise is to predict user behaviour, so these single user events were be aggregated such that in the resulting dataframe each row refers to an individual user.\n",
    "* the columns in the resulting dataframe represent various features, such as page event type, membership level, gender, number of sessions etc, and contain values for these features for each userId.\n",
    "* these values were subsequently be used to create vectors for each user for use in the prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The aggregated dataframe was built up in several steps as follows:\n",
    "1. Obtain the count of each page type for each userId/gender grouping, and pivot this to obtain a table with userId/gender as the row index and ount of page events as columns. To this table an additional column `gender_v` is added that contains the gender data as a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a count of each page type for each userId/SessionId/gender\n",
    "agg_users = event_log_valid \\\n",
    "        .groupby('userId','gender','page') \\\n",
    "        .agg(f.count('page')).withColumnRenamed('count(page)','counts') \\\n",
    "        .sort(f.asc('userId'))\n",
    "# pivoting to a table of userId/SessionId/gender as rows\n",
    "# and count of page events as columns \n",
    "agg_users = agg_users \\\n",
    "    .groupby('userId','gender') \\\n",
    "    .pivot('page') \\\n",
    "    .max('counts') \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn('gender_v',\n",
    "                f.when(f.col('gender')==\"F\", f.lit(0)).otherwise(f.lit(1))\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the earliest timestamp, latest timestamp and number of logged events for each userId/SessionId. Use the difference bewteen these timestamp values to get the duration `online_time` of each session. Aggregate these values by userId to get the earliest timestamp, the last timestamp, the total number of logs and the total `online_time` for each UserId. The earliest and last timestamps were then used to get a value for `half_time` which would be used for trend calulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect timestamp start and information for each sessionId/userId group\n",
    "# calculate online_time duration in hours of each sessionId (1 hour = 60*60*1000 ms)  \n",
    "agg_sessions = event_log_valid \\\n",
    "    .groupby('userId','sessionId').agg(\n",
    "        f.min(f.col('ts')).alias('start_ts'),\n",
    "        f.max(f.col('ts')).alias('end_ts'),\n",
    "        f.countDistinct('ts').alias('total_logs')\n",
    "        ) \\\n",
    "    .withColumn('online_time', ((f.col('end_ts')-f.col('start_ts'))/(1000*60*60)))\n",
    "\n",
    "# aggregate online_time for each user to get total online time\n",
    "# find first and last time stamp for each user\n",
    "# find half time (mid point between first and last time stamp) for each user (to calculate trends)\n",
    "# sum number of sessions for each user\n",
    "agg_sessions =  agg_sessions \\\n",
    "    .groupby('userId').agg(\n",
    "        f.sum(f.col('online_time')).alias('online_time'),\n",
    "        f.min(f.col('start_ts')).alias('start_ts'),\n",
    "        f.max(f.col('end_ts')).alias('end_ts'),\n",
    "        f.countDistinct('sessionId').alias('sessions'),\n",
    "        f.sum(f.col('total_logs')).alias('total_logs')\n",
    "    ) \\\n",
    "    .withColumn('half_time', f.col('start_ts')+((f.col('end_ts')-f.col('start_ts'))/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this step the data is manipulated to provide value for a trend calulation. The earliest timestamp and the number of event logs for each userId/sessionId grouping is aggregated into a dataframe. The `half_time` value for each userId is obtained by joining the relevant column from the `agg_sessions` dataframe created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get earliest timestamp & count logs for each sessionId/UserId,\n",
    "# for use in trend calclation.\n",
    "agg_trends = event_log_valid \\\n",
    "    .groupby('userId','sessionId').agg(\n",
    "        f.min(f.col('ts')).alias('start_ts_trend'),\n",
    "        f.countDistinct('ts').alias('total_logs_trend')\n",
    "    )\n",
    "    \n",
    "# join in half_time value from agg_sessions, compare sessionId start time of with half_time\n",
    "# to split logs into two groups: logs_at_start, logs_at_end \n",
    "agg_trends =  agg_trends \\\n",
    "    .join(agg_sessions.select('userId','half_time'), on = (\"userId\"), how = \"left\") \\\n",
    "    .withColumn('logs_at_start',\n",
    "                f.when(f.col('start_ts_trend') <=f.col('half_time'), f.col('total_logs_trend')).otherwise(f.lit(0))\n",
    "                ) \\\n",
    "    .withColumn('logs_at_end',\n",
    "                f.when(f.col('start_ts_trend') > f.col('half_time'), f.col('total_logs_trend')).otherwise(f.lit(0))\n",
    "                ) \\\n",
    "    .groupby('userId').agg(\n",
    "        f.sum(f.col('logs_at_start')).alias('logs_at_start'),\n",
    "        f.sum(f.col('logs_at_end')).alias('logs_at_end')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The dataframes created in steps 1 to 3 are then joined to create the completed aggregated dataframe. As they will no longer be used they can be deleted to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join agg_sessions agg_gender and agg_trend to aggregation        \n",
    "df_aggregated = agg_users \\\n",
    "    .join(agg_sessions, on=(\"userId\"), how='left') \\\n",
    "    .join(agg_trends, on=(\"userId\"), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn & Downgrade\n",
    "A column `Churn` was added to the table df_agrregated to use as the label for the prediction models. This was achieved using the `Cancellation Confirmation` events to define churn, which applies to both paid and free users. Additionally, a column `downgraded` was added using the `Downgrade` events as a definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define flags\n",
    "flag_cancellation = f.udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "flag_downgrade = f.udf(lambda x: 1 if x == \"Downgrade\" else 0, IntegerType())\n",
    "    \n",
    "# map flags to userIds\n",
    "event_flags = event_log_valid \\\n",
    "    .select('userId','page') \\\n",
    "    .withColumn('churned', flag_cancellation('page')) \\\n",
    "    .withColumn('downgraded', flag_downgrade('page')) \\\n",
    "    .groupby('userId') \\\n",
    "    .agg(\n",
    "        f.max(f.col('churned')).alias('churned'),\n",
    "        f.max(f.col('downgraded')).alias('downgraded')\n",
    "    )\n",
    "\n",
    "# add the columns to the main dataframe\n",
    "df_aggregated = df_aggregated \\\n",
    "        .join(event_flags, on=(\"userId\"), how='left') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data<a name=\"explore2\"></a>\n",
    "* Having defined `churn`, some exploratory data analysis was performed to observe the behavior for users who stayed vs users who churned. \n",
    "* Aggregated values for these two groups of users were explored, comparing the incidence rate that they experienced for specific events in a given time, in this case per hour.\n",
    "* A dictionary of actions `rates_hour` was created, with the events as keys and new *event rate* titles (ending with `_h`) as values.\n",
    "* The dictionary was then looped through to create new feature columns each containing values for the relevant event rate per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining rates per hour:\n",
    "rates_hour = {\n",
    "    'Thumbs Down':'thumbs_down_h',\n",
    "    'Roll Advert':'adverts_h',\n",
    "    'Error':'errors_h',\n",
    "    'Help':'help_h',\n",
    "    'Save Settings':'settings_h',\n",
    "    'Downgrade':'downgrades_h',\n",
    "    'Thumbs Up':'thumbs_up_h',\n",
    "    'Add to Playlist':'playlists_h',\n",
    "    'Add Friend':'freinds_h',\n",
    "    'Upgrade':'upgrades_h',\n",
    "    'NextSong':'songs_h',\n",
    "    'total_logs':'total_logs_h',\n",
    "    'Home':'home_page_h'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in rates_hour.items():\n",
    "    df_aggregated = df_aggregated \\\n",
    "        .withColumn(value, f.col(key) / f.col('online_time')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Addition aggregations were calculated for\n",
    "     - `average session time` per user\n",
    "     - `positivity` - the ratio of positive enjoyment indicators / total enjoyment indicators*\n",
    "     - `negativity` - the ratio of negative enjoyment indicators / total enjoyment indicators*\n",
    "     - `trend` - comparing log_events/hour in the first half of the period the User was active with log_events/hour in the second half\n",
    "\n",
    "(* An enjoyment indicator is a log event that either reflects or would provoke the users level of enjoyment:- `Thumbs Up`, `Add to Playlist`, `Add Friend`, `Upgrade`, `Thumbs Down`, `Roll Advert`, `Error`, `Help`, `Save Settings`, `Downgrade`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional rates\n",
    "df_aggregated = df_aggregated \\\n",
    "    .withColumn('mean_session_time', f.col('online_time') / f.col('sessions')) \\\n",
    "    .withColumn('sum_positive', \n",
    "                f.col('Thumbs Up')+f.col('Add to Playlist')\n",
    "                +f.col('Add Friend')+f.col('Upgrade')) \\\n",
    "    .withColumn('sum_negative', \n",
    "                f.col('Thumbs Down')+f.col('Roll Advert')+f.col('Error')\n",
    "                +f.col('Help')+f.col('Save Settings')+f.col('Downgrade')) \\\n",
    "    .withColumn('positivity', f.col('sum_positive') / (f.col('sum_negative')+f.col('sum_positive')))\\\n",
    "    .withColumn('negativity', f.col('sum_negative') / (f.col('sum_negative')+f.col('sum_positive')))\\\n",
    "    .withColumn('trend', (f.col('logs_at_start')-f.col('logs_at_end'))/ (f.col('online_time')/2))\\\n",
    "    .fillna(0)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The completed aggregated dataframe was sent to Pandas, to facilitate the visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_pd = df_aggregated.toPandas().set_index(\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Data\n",
    "Box plots were chosen to compare the incidence rate of these events between the churned users and the users who stayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining boxplot titles and variables:\n",
    "boxplots = {\n",
    "    'thumbs down / hour':'thumbs_down_h',\n",
    "    'adverts / hour':'adverts_h',\n",
    "    'errors / hour':'errors_h',\n",
    "    'help / hour':'help_h',\n",
    "    'settings / hour':'settings_h',\n",
    "    'downgrades / hour':'downgrades_h',\n",
    "    'thumbs up / hour':'thumbs_up_h',\n",
    "    'added playlists / hour':'playlists_h',\n",
    "    'added freinds / hour':'freinds_h',\n",
    "    'upgrades / hour':'upgrades_h',\n",
    "    'songs / hour':'songs_h',\n",
    "    'logs / hour':'total_logs_h',\n",
    "    'homepage visits / hour':'home_page_h',\n",
    "    'mean session time':'mean_session_time',\n",
    "    'positivity':'positivity',\n",
    "    'negativity':'negativity',\n",
    "    'trend, logs / hour':'trend'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting boxplot variables, stayed vs churned\n",
    "dictlength = len(boxplots)\n",
    "if dictlength%4==0:\n",
    "    val = 0\n",
    "else:\n",
    "    val = 1\n",
    "rows = round(dictlength/4) + val\n",
    "\n",
    "plt.figure(figsize = [24,24])\n",
    "plt.tight_layout()\n",
    "for i in range (0,dictlength):\n",
    "    plt.subplot(rows,4,i+1)\n",
    "    sb.boxplot(data = aggregated_pd, x = 'churned', y = list(boxplots.values())[i], showfliers = False)\\\n",
    "    .set_title(list(boxplots.keys())[i], fontsize=15)    \n",
    "    tick_names = ['Stayed','Churned']\n",
    "    plt.xticks([0,1],tick_names)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')    \n",
    "plt.savefig('boxplot_rate.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heatmap was drawn to study the correlation between the incidence rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_pd = aggregated_pd[list(boxplots.values())]\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "sb.heatmap(heatmap_pd.corr(), annot=True, fmt=\".2f\", ax=ax, cmap=\"vlag\", center=0)\\\n",
    ".set_title('Heatmap showing Correlation Between Numeric Features', fontsize=15)\n",
    "plt.savefig('heatmap_pd.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barchart comparing number of users who churned with those who stayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_names = ['Staying Members','Churned Members']\n",
    "plt.xticks([0,1],tick_names)\n",
    "sb.countplot(data = aggregated_pd, x = 'churned').set_title('Dataset Inbalance', fontsize=15)\n",
    "plt.xticks([0,1],tick_names)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('count')\n",
    "plt.savefig('inbalance.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Exploratory data Analysis\n",
    "* reference to the box plot diagrams shows that the median and quartile values for several of these incidence rates differ for users who have stayed and those who have churned, particluarly for `thumbs down / hour`, `thumbs up / hour``adverts / hour`, `mean session time / hour`, `positivity` and `negativity`.\n",
    "* however it should be noted that there is also a lot overlap between the distributions, which could lead to uncertainties when making a prediction.\n",
    "* regarding the heatmap, some of the features have a very strong correlation bewtween them, which means that they could be considered duplicate and can therefore be discarded: \n",
    "    - `positivity` and `negativity` unsurprisingly have a very strong negative correlation, so `negativity` can be discarded.\n",
    "    - `total_logs_h` and `songs_h` are also positively related, as are `total_logs_h` and `adverts_h`, together with `total_logs_h` and `home_page_h`. Seeing as Sparkify is about playing songs, `total_logs_h` and `home_page_h` could also be dropped.\n",
    "* Regarding the ratio of churned to stayed users, the data is not balanced, so `Balanced Accuracy` should be used when evaluating the prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management:\n",
    "Before proceeding the various supporting pandas and spark dataframes were deleted, to free up RAM for the Modelling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_list = [\n",
    "    pages_agg_pd, \n",
    "    gender_agg_pd, \n",
    "    gender_level_agg_pd,\n",
    "    aggregated_pd,\n",
    "    heatmap_pd\n",
    "]\n",
    "del pandas_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log.unpersist()\n",
    "event_log_valid.unpersist()\n",
    "pages_agg.unpersist()\n",
    "gender_agg.unpersist()\n",
    "gender_level_agg.unpersist()\n",
    "agg_users.unpersist()\n",
    "agg_sessions.unpersist()\n",
    "agg_trends.unpersist()\n",
    "event_flags.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering<a name=\"feature\"></a>\n",
    "The features which will used with those which were studied in the Boxplot and Heatmap anaylses minus the \"correlated duplicates\".\n",
    "* `thumbs_down_h`: thumbs down / hour\n",
    "* `adverts_h`: adverts / hour\n",
    "* `errors_h`: errors / hour\n",
    "* `help_h`: visits to help pages / hour\n",
    "* `settings_h`: number of changes to user settings / hour\n",
    "* `downgrades_h`: 'downgrades / hour'\n",
    "* `thumbs_up_h`: 'thumbs up / hour'\n",
    "* `playlists_h`: songs added to playlists / hour\n",
    "* `freinds_h`: recommendations to freinds / hour\n",
    "* `upgrades_h`: upgrades / hour\n",
    "* `songs_h`: songs playyed / hour\n",
    "* `mean_session_time`: average length of session time duration\n",
    "* `positivity`\n",
    "* `trend`: trend for rate of logs / hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify numeric features\n",
    "cols_features = list(boxplots.values())\n",
    "cols_features.remove('total_logs_h')\n",
    "cols_features.remove('home_page_h')\n",
    "cols_features.remove('negativity')\n",
    "\n",
    "# identify binary features\n",
    "bin_features = ['gender_v','downgraded']\n",
    "\n",
    "# define VectorAssembler combining numeric features into vector \"NumericFeatures\"\n",
    "numeric_assembler = VectorAssembler(inputCols=cols_features, outputCol=\"NumericFeatures\")\n",
    "\n",
    "# define Normalization of \"NumericFeatures\" to \"ScaledFeatures\" vector\n",
    "numeric_scaler = Normalizer(inputCol=\"NumericFeatures\", outputCol=\"ScaledFeatures\")\n",
    "\n",
    "# define VectorAssembler to add binary features to \"ScaledFeatures\" to obtain Vector \"features\"\n",
    "final_assembler = VectorAssembler(inputCols=bin_features + [\"ScaledFeatures\"], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of dataframe containing the features that are to be processed\n",
    "# and test the functions for plausibility and errors\n",
    "df_test = df_aggregated.select(*bin_features,*cols_features)\n",
    "\n",
    "# Assemble Numeric Features into vector \"NumericFeatures\"\n",
    "df_test = numeric_assembler \\\n",
    "    .transform(df_test) \\\n",
    "    .drop(*cols_features)\n",
    "\n",
    "# Normalize \"NumericFeatures\" as \"ScaledFeatures\"\n",
    "df_test = numeric_scaler.transform(df_test)\n",
    "\n",
    "# Add binary features to \"ScaledFeatures\" to obtain Vector \"features\"\n",
    "df_test = final_assembler.transform(df_test)\n",
    "\n",
    "# check output\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling<a name=\"model\"></a>\n",
    "* The full dataset was copied and split into train, and test sets.\n",
    "* Three machine learning methods `LogisticRegression`, `RandomForestClassifier`and `GBTClassifier` were tried.\n",
    "* These were evaluated for accuracy of the various models, and the parameters tuned as necessary.\n",
    "* Based on test accuracy the winning model was determined and results on the validation set were reported.\n",
    "* Since the churned users are a fairly small subset, F1 score was the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Copy dataframe; Apply Train Test Split<a name=\"copy\"></a>\n",
    "* `df_model`, a copy of the dataframe `df_aggregated` was made and then randomly split into 90% for training and 10% for testing,<br> with the random seed set to `42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy aggregated to new dataframe df_model, set column 'churned' as label\n",
    "df_model = df_aggregated\\\n",
    "    .select(*bin_features,*cols_features,'churned')\\\n",
    "    .withColumnRenamed('churned','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (20% held out for testing)\n",
    "(trainingData, testData) = df_model.randomSplit([0.9, 0.1], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Build Pipelines<a name=\"pipes\"></a>\n",
    "* Three pipelines have been built using the algorythmns `LogisticRegression`, `RandomForestClassifier`and `GBTClassifier` (Gradient Boosted Tree), initially with default parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_build(model):\n",
    "    \"\"\"\n",
    "    Function to build pipeline using engineered features\n",
    "    In:\n",
    "    - name of model\n",
    "    Out:\n",
    "    - pipeline\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(stages=[numeric_assembler,numeric_scaler,final_assembler, model])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",maxIter=100,regParam=0.0,elasticNetParam=0.0)\n",
    "pipeline_lr =  pipeline_build(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=5, numTrees=20)\n",
    "pipeline_rfc =  pipeline_build(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT Classifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=5, maxIter=20)\n",
    "pipeline_gbt = pipeline_build(gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train Base Models (default Parameter settings) and Make First Predictions<a name=\"train\"></a>\n",
    "### 3.1 Function for Generating Prediction using Model with Default Parameters:\n",
    "* This function is written to train a given Pipeline model with its default Parameter settings, using the trainingData from the data split.\n",
    "* The trained model is then used to make a prediction for the remaining testData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(pipeline):\n",
    "    \"\"\"\n",
    "    Function to train basic pipeline models with default parameter settings and make predictions\n",
    "    In:\n",
    "    - pipeline\n",
    "    Out:\n",
    "    - prediction\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model = pipeline.fit(trainingData)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(testData)\n",
    "    \n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run function for the three Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic Regression\n",
    "base_prediction_lr = prediction(pipeline_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "base_prediction_rfc = prediction(pipeline_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT Classifier\n",
    "base_prediction_gbt = prediction(pipeline_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluate First Predictions<a name=\"evaluate\"></a>\n",
    "### 4.1 Function for Calculating Accuracy Values for given Predictions:\n",
    "* with reference to the [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) definition in wikipedia, together with Spark's `BinaryClassificationEvaluator()`, a function was written to obtain values first for a confusion matrix, which could be used to calulate the Accuracy and F1 Score, as well as values for Area under the ROC and PR curves.\n",
    "* Because the data is not balanced, due to the number of churned members being far less than the number of staying members, `Balanced Accuracy` should be used for evaluating the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Function to test accuracy of prediction\n",
    "    In:\n",
    "    - prediction dataframe\n",
    "    Out:\n",
    "    - dictionary containing results showing accuracy of prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # confusion matrix for accuracy and f1 calculation\n",
    "    positives = prediction.filter(prediction.label == 1)\n",
    "    true_pos = (positives.filter(positives.label == positives.prediction).count())\n",
    "    fals_neg = (positives.filter(positives.label != positives.prediction).count())\n",
    "    negatives = prediction.filter(prediction.label == 0)\n",
    "    true_neg = (negatives.filter(negatives.label == negatives.prediction).count())\n",
    "    fals_pos = (negatives.filter(negatives.label != negatives.prediction).count())\n",
    "    \n",
    "    # balanced accuracy\n",
    "    tpr = true_pos/(true_pos+fals_neg)\n",
    "    tnr = true_neg/(true_neg+fals_pos)\n",
    "    accuracy = round((tpr+tnr)/2,3)\n",
    "    \n",
    "    # f1 score\n",
    "    f1_score = round(2*true_pos/(2*true_pos+fals_pos+fals_neg),3)\n",
    "    \n",
    "    # area under curve evaluation using API BinaryClassificationEvaluator\n",
    "    roc_evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "    pr_evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
    "    \n",
    "    au_roc = round(roc_evaluator.evaluate(prediction),3)\n",
    "    au_pr = round(pr_evaluator.evaluate(prediction),3)\n",
    "    \n",
    "    results = {\n",
    "        'True Positives':true_pos,\n",
    "        'False Negatives':fals_neg,\n",
    "        'True Negatives':true_neg,\n",
    "        'False Positives':fals_pos,\n",
    "        'Balanced Accuracy':accuracy,\n",
    "        'F1 Score':f1_score,\n",
    "        'Area under PR':au_pr,\n",
    "        'Area under ROC':au_roc\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Run the Function to Compute Accuracy\n",
    "#### 4.2a LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_result_lr = test_prediction(base_prediction_lr)\n",
    "base_result_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2b RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_result_rfc = test_prediction(base_prediction_rfc)\n",
    "base_result_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2c GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_result_gbt = test_prediction(base_prediction_gbt)\n",
    "base_result_gbt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Summary of Results and Insights for Default Models\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to facilitate this a function for creating tables in matplotlib is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate the results\n",
    "base_result_pd_lr = pd.DataFrame.from_dict(base_result_lr, orient='index', columns=['LogisticRegression_Base'])\n",
    "base_result_pd_rfc = pd.DataFrame.from_dict(base_result_rfc, orient='index', columns=['RandomForestClassifier_Base'])\n",
    "base_result_pd_gbt = pd.DataFrame.from_dict(base_result_gbt, orient='index', columns=['GBTClassifier_Base'])\n",
    "\n",
    "base_result_pd = base_result_pd_lr\\\n",
    "    .join(base_result_pd_rfc, how='left')\\\n",
    "    .join(base_result_pd_gbt, how='left')\n",
    "\n",
    "base_result_pd.index.names = [' ']\n",
    "base_result_pd.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = render_mpl_table(base_result_pd, header_columns=0, col_width=5.0)\n",
    "fig.savefig(\"base_result_pd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "* Area under the curves for all models is relatively high thanks to their 100% success rate when predicting true negatives, however this may well be due to the significant inbalance in the data between positives and negatives.\n",
    "* Balanced accuracy and F1 score are not so optimistic, owing to the fact that true Positives are not so well predicted. In fact the Random Forest Classifier model failed to predict any true Positives at all.\n",
    "* Based on the initial assessment, GBT Classifier has an edge with a higher F1 Score and Balanced Accuracy, since it was able to identify 2 from 4 Postitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Tune Models and Re-Compute Accuracy<a name=\"tune\"></a>\n",
    "* A Function for executing 3-fold cross-validation on a given Pipeline Model was written, which takes the base pipeline model and  Parameter grid, crossvalidates it using the Spark Tuning Library `CrossValidator` and `trainData` to find the most accurate models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Defining Function for Tuning the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(pipeline, paramGrid):\n",
    "    \"\"\"\n",
    "    Function to cross validate parameters of pipeline models\n",
    "    to obtain best model\n",
    "    In:\n",
    "    - pipeline to optimize\n",
    "    - paramGrid relevant to pipeline\n",
    "    Out:\n",
    "    - tuned model with optimized parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          parallelism=4)\n",
    "    \n",
    "    cvModel = crossval.fit(trainingData)\n",
    "    \n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Defining Parameter Values for Tuning the Models\n",
    "* In order to use this function, Parameter Grids were defined. \n",
    "* First the default allowed values parameter values for the pipeline models were obtained from the Spark documentation for [LogisticRegression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html), [RandomForestClassifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html) and [GBTClassifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.GBTClassifier.html)\n",
    "* Based on this information parameterGrids were built for the cross validation as follows:\n",
    "\n",
    "| **Model** | **Main Parameters** | **Default values** | **CrossValidation Values** | \n",
    "| :---------- | :---------- | :---------- | :---------- |\n",
    "| LogisticRegression | maxIter<br>regParam<br>elasticNetParam | 100<br>0.0<br>0.0 | 100, 50, 20<br>0.0, 10.0, 20.0<br>0.0, 1.0 | \n",
    "| RandomForestClassifier | maxDepth<br>numTrees | 5<br>20 | 5, 3, 7<br>20, 10, 40 |\n",
    "| GBTClassifier | maxDepth<br>maxIter | 5<br>20 |  5, 3, 7<br>20, 10, 40  |\n",
    "\n",
    "* The `.avgmetrics` for the cross validations was obtained to identify the best parameter combination.\n",
    "* The pipelines wer then reconstructed with these parameters, and the predictions re-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cross Validate and Tune the Models, Run the Predictions and Compute Accuracy\n",
    "#### 5.3a LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ParamGrid\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter,[100,50,200]) \\\n",
    "    .addGrid(lr.regParam,[0.0, 10.0, 20.0]) \\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate Model\n",
    "cv_model_lr = crossval(pipeline_lr, paramGrid_lr)\n",
    "cv_model_lr.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results for LogisticRegression**\n",
    "\n",
    "| **Main Parameters** | **Default** | **Grid Search** | <nbsp> | <nbsp> |**best.avgMetrics** | **Values for best** |\n",
    "| :---------- | :---------- | :---------- | :---------- | :---------- | :---------- | :---------- |\n",
    "| maxIter<br>regParam<br>elasticNetParam | 100<br>0.0<br>0.0 | 100<br>0.0<br>0.0 | 50<br>10.0<br>1.0 | 200<br>20.0<br>- | 0.5496865704561543 | 50, 100, or 200 <br>20.0<br>0.0 or 1.0 |\n",
    "\n",
    "* select `maxIter`=100, `regParam`=20.0; `elascticNetParam`=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Model\n",
    "lr_tuned = LogisticRegression(maxIter=100,regParam=20.0,elasticNetParam=0.0)\n",
    "tuned_model_lr =  pipeline_build(lr_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction\n",
    "tuned_prediction_lr = prediction(tuned_model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results\n",
    "tuned_result_lr = test_prediction(tuned_prediction_lr)\n",
    "tuned_result_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3b RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ParamGrid\n",
    "paramGrid_rfc = ParamGridBuilder() \\\n",
    "    .addGrid(rfc.maxDepth,[5, 3, 7]) \\\n",
    "    .addGrid(rfc.numTrees,[20,10,40]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate Model\n",
    "cv_model_rfc = crossval(pipeline_rfc, paramGrid_rfc)\n",
    "cv_model_rfc.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results for RandomForestClassifier**\n",
    "\n",
    "| **Main Parameters** |**Default** | **Grid Search** | <nbsp> | <nbsp> | **best.avgMetrics** | **Params for best.avgMetrics** |\n",
    "| :---------- | :---------- | :---------- | :---------- | :---------- | :---------- | :---------- |\n",
    "| maxDepth<br>numTrees | 5<br>20 | 5<br>20 | 3<br>10 | 7<br>40 | 0.5917140937445151 |  5<br>10 |\n",
    "\n",
    "* select `maxDepth`=5, `numTrees`=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Model\n",
    "rfc_tuned = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=5, maxDepth=10)\n",
    "tuned_model_rfc =  pipeline_build(rfc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction\n",
    "tuned_prediction_rfc = prediction(tuned_model_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results\n",
    "tuned_result_rfc = test_prediction(tuned_prediction_rfc)\n",
    "tuned_result_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3c GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ParamGrid\n",
    "#paramGrid_gbt = ParamGridBuilder() \\\n",
    "#    .addGrid(gbt.maxDepth,[5, 3, 7]) \\\n",
    "#    .addGrid(gbt.maxIter,[20, 10, 40]) \\\n",
    "#    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate Model\n",
    "# cv_model_gbt = crossval(pipeline_gbt, paramGrid_gbt)\n",
    "# cv_model_gbt.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** durung the development of this project it was found that cross-validation for GBTClassifier was very resource intensive, with several hours passing without result when running a cross-validation of 3 values for each of 2 parameters, even when setting the parallism  parameter to a value higher than default 1. To expediate tuning, it was decided to abandon cross-validation for GBTClassifier and find the best result by manual trial of the grid search parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Optimisation for GBTClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GBTClassifier Model Candidates:\n",
    "candidate_1 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=5,maxIter=10)\n",
    "candidate_2 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=5,maxIter=40)\n",
    "candidate_3 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=3,maxIter=20)\n",
    "candidate_4 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=3,maxIter=10)\n",
    "candidate_5 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=3,maxIter=40)\n",
    "candidate_6 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=7,maxIter=20)\n",
    "candidate_7 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=7,maxIter=10)\n",
    "candidate_8 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth=7,maxIter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build trial Pipelines for each Candidate\n",
    "trial_model_1 =  pipeline_build(candidate_1)\n",
    "trial_model_2 =  pipeline_build(candidate_2)\n",
    "trial_model_3 =  pipeline_build(candidate_3)\n",
    "trial_model_4 =  pipeline_build(candidate_4)\n",
    "trial_model_5 =  pipeline_build(candidate_5)\n",
    "trial_model_6 =  pipeline_build(candidate_6)\n",
    "trial_model_7 =  pipeline_build(candidate_7)\n",
    "trial_model_8 =  pipeline_build(candidate_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 1\n",
    "trial_prediction_1 = prediction(trial_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 2\n",
    "trial_prediction_2 = prediction(trial_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 3\n",
    "trial_prediction_3 = prediction(trial_model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 4\n",
    "trial_prediction_4 = prediction(trial_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 5\n",
    "trial_prediction_5 = prediction(trial_model_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 6\n",
    "trial_prediction_6 = prediction(trial_model_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 7\n",
    "trial_prediction_7 = prediction(trial_model_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Prediction Try 8\n",
    "trial_prediction_8 = prediction(trial_model_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 1\n",
    "test_result_1 = test_prediction(trial_prediction_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 2\n",
    "test_result_2 = test_prediction(trial_prediction_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 3\n",
    "test_result_3 = test_prediction(trial_prediction_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 4\n",
    "test_result_4 = test_prediction(trial_prediction_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 5\n",
    "test_result_5 = test_prediction(trial_prediction_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 6\n",
    "test_result_6 = test_prediction(trial_prediction_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 7\n",
    "test_result_7 = test_prediction(trial_prediction_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure Results Try 8\n",
    "test_result_8 = test_prediction(trial_prediction_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results from GBTClassifier Trials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate the results\n",
    "tuning_gbt_0_pd = pd.DataFrame.from_dict(base_result_gbt, orient='index', columns=['Default 5,20'])\n",
    "tuning_gbt_1_pd = pd.DataFrame.from_dict(test_result_1, orient='index', columns=['Trial_1 5,10'])\n",
    "tuning_gbt_2_pd = pd.DataFrame.from_dict(test_result_2, orient='index', columns=['Trial_2 5,40'])\n",
    "tuning_gbt_3_pd = pd.DataFrame.from_dict(test_result_3, orient='index', columns=['Trial_3 3,20'])\n",
    "tuning_gbt_4_pd = pd.DataFrame.from_dict(test_result_4, orient='index', columns=['Trial_4 3,10'])\n",
    "tuning_gbt_5_pd = pd.DataFrame.from_dict(test_result_5, orient='index', columns=['Trial_5 3,40'])\n",
    "tuning_gbt_6_pd = pd.DataFrame.from_dict(test_result_6, orient='index', columns=['Trial_6 7,20'])\n",
    "tuning_gbt_7_pd = pd.DataFrame.from_dict(test_result_7, orient='index', columns=['Trial_7 7,10'])\n",
    "tuning_gbt_8_pd = pd.DataFrame.from_dict(test_result_8, orient='index', columns=['Trial_8 7,40'])\n",
    "\n",
    "tuning_gbt_pd = tuning_gbt_0_pd\\\n",
    "    .join(tuning_gbt_1_pd, how='left')\\\n",
    "    .join(tuning_gbt_2_pd, how='left')\\\n",
    "    .join(tuning_gbt_3_pd, how='left')\\\n",
    "    .join(tuning_gbt_4_pd, how='left')\\\n",
    "    .join(tuning_gbt_5_pd, how='left')\\\n",
    "    .join(tuning_gbt_6_pd, how='left')\\\n",
    "    .join(tuning_gbt_7_pd, how='left')\\\n",
    "    .join(tuning_gbt_8_pd, how='left')\n",
    "\n",
    "tuning_gbt_pd.index.names = [' ']\n",
    "tuning_gbt_pd.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = render_mpl_table(tuning_gbt_pd, header_columns=0, col_width=2.7)\n",
    "fig.savefig(\"tuning_gbt_pd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select Best Model based on Results from GBTClassifier Trials**\n",
    "\n",
    "| **Main Parameters** | **Default** | **Grid Search** | <nbsp> | <nbsp> |  **Params for best** |\n",
    "| :---------- | :---------- | :---------- | :---------- | :---------- | :---------- |\n",
    "| maxDepth<br>maxIter | 5<br>20 | 5<br>20 | 3<br>10  | 7<br>40 | 3<br>40  |\n",
    "\n",
    "* select `maxDepth`=3, `maxIter`=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best Result\n",
    "tuned_result_gbt = test_result_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Summary of Results and Insights for Tuned Models\n",
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate the results\n",
    "tuned_result_pd_lr = pd.DataFrame.from_dict(tuned_result_lr, orient='index', columns=['LogisticRegression_Tuned'])\n",
    "tuned_result_pd_rfc = pd.DataFrame.from_dict(tuned_result_rfc, orient='index', columns=['RandomForestClassifier_Tuned'])\n",
    "tuned_result_pd_gbt = pd.DataFrame.from_dict(tuned_result_gbt, orient='index', columns=['GBTClassifier_Tuned'])\n",
    "\n",
    "tuned_result_pd = tuned_result_pd_lr\\\n",
    "    .join(tuned_result_pd_rfc, how='left')\\\n",
    "    .join(tuned_result_pd_gbt, how='left')\n",
    "\n",
    "tuned_result_pd.index.names = [' ']\n",
    "tuned_result_pd.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = render_mpl_table(tuned_result_pd, header_columns=0, col_width=5.0)\n",
    "fig.savefig(\"tuned_result_pd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "* Area under the curves for LogisticRegression improved when using the best Paramater Set from the CrossValidation. However the F1 Score and Balanced Accuracy increased.\n",
    "* Area under the curves for RandomForestClassifier decreased when using the best Paramater Set from the CrossValidation. However the F1 Score and Balanced Accuracy increased.\n",
    "* Area under the curves for GBTClassifier decreased when using the best Paramater Set from the CrossValidation. However the F1 Score and Balanced Accuracy increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretations\n",
    "The results of the CrossValidation are counter-intuitive, in that the so-called best models are performing less well for some indicators compared to the default values, which were also included in the Parameter Grid.However the with the manual process used on GBTClassifier it was possible to find some parameters that led to an all round improvement.<br><br>\n",
    "It should be remembered however that the `.avgMetrics` that are given are average values measured over the 3 folds of the cross validation.<br><br>\n",
    "Each fold uses a different subset of the training data. The data has already been shown to be biased, and depending on how these subsets are selected, the bias could be exaggerated, leading to one or more subsets with an unrepresentative proportion of _churn_:_stayed users_, or a difference in this proportion between the subsets. Either way the model will be learning differently for each fold.<br><br>\n",
    "If one of the folds then delivers an abnormally low value because of this mismatch, it will effect the average in such a way that a misleading `.avgMetric` is given, which in turn leads to an incorrect selection of parameter when tuning.\n",
    "The training data used in the cross validation consists of 90% of the aggregated dataframe approximately 200 rows, but the testing data is much smaller at 10% or only about 25 rows. This is a very small sample size with a very low statistical significance, which could lead to different outputs each time the prediction is run.<br><br>\n",
    "The metrics also only apply to the evaluators definitions, in this case `BinaryClassificationEvaluator` which only considers two metrics, Area under PR and Area under ROC. `F1 Score` has been measured \"manually\".<br><br>\n",
    "This could be a reason why the tuned models are performing less well for some indicators but better for others, when compared to the default models.<br><br>\n",
    "With the manual process used to optimise GBTClassifier, no such subsetting took place, which is equivalent to only one fold taking place. Whilst this delivers a desirable result with improved accuracy values for the prediction, it does not guarentee that the result is repeatable, particularly when applying it to a prediction based on the full 12GB dataset. This would need to be verified with a trial by running the prediction as a Spark cluster on the cloud using AWS or IBM Cloud.<br><br>\n",
    "An additional point to consider is that we are checking our prediction of userIds who will churn against a list of `userIds` who did churn. It is possible, that some of the false positives in the prediction aren't actually false. The data in the sample was collected over months, which is a relatively short period of time. It could be that the model has indeed identified a user who will churn, but who has not done so yet, according to the sample data, because of the short collection period. A false positive may not necessarily be a bad thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "# Recommended Model and Further Work\n",
    "Based on the results such that they are the recommendation is to use the GBTClassifier Model, with Hyperparameters `maxDepth` and `maxIter` tuned to 3 and 40 respectively.<br>\n",
    "It out performs both other models in the default settings as well as appearing to improve with tuning<br>\n",
    "However, for the reasons discussed above in _Interpretations_, before deploying the model into production it should first be verified with the full data set using a Spark cluster in the cloud, to prove its reliabilty, if necessary taking steps to tune it further.\n",
    "Nevertheless it has the potential to deliver a solution for predicting customer _churns_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
